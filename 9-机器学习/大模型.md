## 语义理解的挑战

NLP（自然语言处理）主要会面临以下的问题和挑战：

- 门把手弄坏了 【门（把手弄坏了）/ （门把手）弄坏了。】 -> 切分歧义
- 喜欢乡下的孩子 【（喜欢乡下）的孩子 / 喜欢（乡下的孩子）。】-> 结构歧义
- 老虎苍蝇一起打 【『老虎』，『苍蝇』均有特质。】 -> 语义歧义
- 文本中始终存在未知语言现象：新的语义，新的含义，新的用法。类似于中国一些梗名词（小黑子），和英文的新单词。
- 文本自带的一些文化，历史背景导致的整体句子的语义变化。

现阶段，使用大模型技术就是一把梭，然后看一下能否满足上述能力。

## NLP的进化史

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/3851847.png)

## 神经网络

将神经元的活动，使用数学进行描述：

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/4040056.png)

激活函数有很多种，最常用的有：sigmoid函数，Tanh函数，ReLU函数等：

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/8280775.png)

### 前馈神经网络（多层感知器）

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/6264238.png)

整个信息传播过程可以用右边的函数表示。这里的 f 是激活函数，w是权重（权重矩阵），b是偏置。

这里提到了全连接，我们看一下什么是全连接，什么是非全连接。

#### 神经网络的全连接和非全连接

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/8926514.png)

#### 前馈神经网络中的净输入和活性值指的什么

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/5450925.png)

#### 交叉熵的概念和其作用

在前馈神经网络中，交叉熵通常被用作**损失函数**，用于衡量模型的预测结果与真实值之间的差异。
‎
在分类问题中，神经网络的输出通常是每个类别的预测概率，而真实值则是每个类别的实际概率（通常用one-hot编码表示）。交叉熵损失函数就是用来衡量这两个概率分布之间的差异的。
‎
具体来说，假设模型对某个样本的预测概率分布为p，真实概率分布为q，那么交叉熵损失函数可以表示为：
‎
`H(p, q) = - Σ q(i) log p(i)`
‎
其中，Σ表示对所有类别求和，q(i)是真实概率分布中类别i的概率，p(i)是预测概率分布中类别i的概率。
‎
通过最小化交叉熵损失函数，我们可以使模型的预测结果尽可能接近真实值，从而提高模型的性能。

在训练神经网络时，我们的目标是**最小化损失函数**。为了达到这个目标，我们需要调整神经网络的权重和偏置。这个调整过程通常通过**梯度下降**（Gradient Descent）或其变种（如随机梯度下降，Adam等）来完成。

在梯度下降中，我们首先计算损失函数关于权重和偏置的梯度，然后按照梯度的反方向更新权重和偏置。这个过程会反复进行，直到损失函数达到最小值。

所以，虽然交叉熵不能直接用来计算权重和偏置，但它是通过梯度下降来调整权重和偏置的关键步骤。

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/8455463.png)

#### 基于前馈神经网络的语言模型（N元文法模型）

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/1336467.png)

这里说的长期依赖主要是两点：一个是FNN每次输入都是独立的，不能依赖之前的输入；二是FNN要求输入和输出的维度是固定的，但往往时序数据（如语音，文本）的长度不固定。

这里还提到了对数似然，什么是对数似然？

#### 对数似然

![](https://aeiblog-1301396258.cos.ap-chengdu.myqcloud.com/img/626327.png)